{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SVN NOTICE\n",
    "Do not run this on normal computer (Time Wasted: > 21 hours)"
   ],
   "id": "5c9b5b03b6a36daa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "import time\nfrom typing import Any\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, \\\n    recall_score, f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.svm import SVC\n\n# Things that belong to strongly predictions, we just remove this\nexclusion_list = ['num_outbound_cmds', 'attack_category', 'success_pred', 'attack_type']\n\n# Define nominal and binary columns\n# numeric is calculated using above 2.\nnominal = ['protocol_type', 'service', 'flag']\nbinary = ['land', 'logged_in', 'root_shell', 'su_attempted', 'is_host_login', 'is_guest_login']\n\n\ndef define_numeric_column(data: pd.DataFrame) -> list[Any]:\n    # Define numeric columns dynamically based on the dataframe\n    return [col for col in data.columns if col not in nominal + binary + exclusion_list]\n\n\ndef print_score():\n    # Calculate other evaluation metrics for test set\n    precision = precision_score(y_test, y_test_pred, average='weighted')\n    recall = recall_score(y_test, y_test_pred, average='weighted')\n    f1 = f1_score(y_test, y_test_pred, average='weighted')\n    print(f\"SIMPLE SVM CLASSIFIER\")\n    print(\"===========================================================================\")\n    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred))\n    print(\"Decision Tree Model Evaluation:\")\n    print(f\"Training Time: {training_time:.4f} seconds\")\n    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n    print(f\"Precision (Weighted): {precision:.4f}\")\n    print(f\"Recall (Weighted): {recall:.4f}\")\n    print(f\"F1-Score (Weighted): {f1:.4f}\")\n\n    # Print confusion matrix with class labels\n    conf_matrix = confusion_matrix(y_test, y_test_pred)\n    class_names = np.unique(y)\n    print(\"\\nConfusion Matrix with Class Labels:\")\n    print(pd.DataFrame(conf_matrix, index=class_names, columns=class_names))\n\n\ndef check_fitting():\n    print(\"\\nOverfitting/Underfitting Diagnosis:\")\n    if train_accuracy > test_accuracy + 0.05:  # Arbitrary threshold (e.g., 5% gap)\n        print(\"Possible Overfitting: Training accuracy is significantly higher than testing accuracy.\")\n    elif train_accuracy < 0.7 and test_accuracy < 0.7:  # Arbitrary low threshold\n        print(\"Possible Underfitting: Both training and testing accuracies are low.\")\n    else:\n        print(\"Good Fit: Training and testing accuracies are reasonably close.\")\n\n\ndef preprocess_data(data: pd.DataFrame) -> tuple:\n    data = data.copy()\n    # Data cleaning\n    data['su_attempted'] = data['su_attempted'].replace({2: 0}, inplace=False)\n    # Define columns\n    numeric = define_numeric_column(data)\n    return data[nominal + numeric + binary], data['attack_category']\n\n\ndef label_encode_nominal_data(features: pd.DataFrame):\n    for col in nominal:\n        le = LabelEncoder()\n        features[col] = le.fit_transform(features[col])\n\n\ndf = pd.read_csv('./data/kdd_merged.csv')\nX, y = preprocess_data(df)\n\nlabel_encode_nominal_data(X)\n\n# Scale numerical features (KNN is distance-based)\nnumeric_cols = define_numeric_column(df)\nscaler = StandardScaler()\nX[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Feature selection - Select top 20 features\nselector = SelectKBest(score_func=mutual_info_classif, k=20)\nX_train_selected = selector.fit_transform(X_train, y_train)\nX_test_selected = selector.transform(X_test)\n\nparam_grid = {\n    'C': [0.1, 1, 10, 100],  # Regularization parameter values\n    'kernel': ['linear'],  # Fixed to linear kernel as requested\n}\n\n# Initialize the SVC model\ndt_model = SVC(random_state=42)\n\n# Set up GridSearchCV with f1-weighted scoring\ngrid_search = GridSearchCV(estimator=dt_model,\n                           param_grid=param_grid,\n                           cv=5,  # 5-fold cross-validation\n                           scoring='f1_weighted',  # Focus on f1-weighted\n                           n_jobs=-1,  # Use all available CPU cores\n                           verbose=1)  # Show progress\n\n# Train with hyperparameter tuning\nstart_time = time.time()\ngrid_search.fit(X_train_selected, y_train)  # Use selected features\ntraining_time = time.time() - start_time\n\n# Get the best model\nbest_dt_model = grid_search.best_estimator_\nprint(\"\\nBest Hyperparameters:\", grid_search.best_params_)\nprint(f\"Hyperparameter Tuning Time: {training_time:.4f} seconds\")\n\n# Predict with the best model\ny_train_pred = best_dt_model.predict(X_train_selected)\ny_test_pred = best_dt_model.predict(X_test_selected)\n\n# Calculate accuracies\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\n# Call your existing functions\nprint_score()\ncheck_fitting()\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
