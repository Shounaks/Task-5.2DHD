{
 "cells": [
  {
   "cell_type": "code",
   "id": "46b3ee6743ef45ec",
   "metadata": {},
   "source": [
    "import time\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, \\\n",
    "    recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Exclusion list and column definitions\n",
    "exclusion_list = ['num_outbound_cmds', 'attack_category', 'success_pred', 'attack_type']\n",
    "nominal = ['protocol_type', 'service', 'flag']\n",
    "binary = ['land', 'logged_in', 'root_shell', 'su_attempted', 'is_host_login', 'is_guest_login']"
   ],
   "id": "ffde21ee1203de3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def define_numeric_column(data: pd.DataFrame) -> list[Any]:\n",
    "    return [col for col in data.columns if col not in nominal + binary + exclusion_list]\n",
    "\n",
    "def print_score(y_test, y_test_pred, training_time, train_accuracy, test_accuracy):\n",
    "    precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    print(f\"SIMPLE DECISION TREE with Hyperparameter Tuning\")\n",
    "    print(\"===========================================================================\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "    print(\"Decision Tree Model Evaluation:\")\n",
    "    print(f\"Training Time: {training_time:.4f} seconds\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Precision (Weighted): {precision:.4f}\")\n",
    "    print(f\"Recall (Weighted): {recall:.4f}\")\n",
    "    print(f\"F1-Score (Weighted): {f1:.4f}\")\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    class_names = np.unique(y)\n",
    "    print(\"\\nConfusion Matrix with Class Labels:\")\n",
    "    print(pd.DataFrame(conf_matrix, index=class_names, columns=class_names))\n",
    "\n",
    "def check_fitting(train_accuracy, test_accuracy):\n",
    "    print(\"\\nOverfitting/Underfitting Diagnosis:\")\n",
    "    if train_accuracy > test_accuracy + 0.05:\n",
    "        print(\"Possible Overfitting: Training accuracy is significantly higher than testing accuracy.\")\n",
    "    elif train_accuracy < 0.7 and test_accuracy < 0.7:\n",
    "        print(\"Possible Underfitting: Both training and testing accuracies are low.\")\n",
    "    else:\n",
    "        print(\"Good Fit: Training and testing accuracies are reasonably close.\")\n",
    "\n",
    "def preprocess_data(data: pd.DataFrame) -> tuple:\n",
    "    try:\n",
    "        data = data.copy()\n",
    "        data['su_attempted'] = data['su_attempted'].replace({2: 0}, inplace=False)\n",
    "        numeric = define_numeric_column(data)\n",
    "        return data[nominal + numeric + binary], data['attack_category']\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error in preprocessing: {str(e)}\")\n",
    "\n",
    "def label_encode_nominal_data(features: pd.DataFrame):\n",
    "    for col in nominal:\n",
    "        le = LabelEncoder()\n",
    "        features[col] = le.fit_transform(features[col])"
   ],
   "id": "fac1882b2e4c6796"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load and preprocess data\n",
    "df = pd.read_csv('./data/kdd_merged.csv')\n",
    "X, y = preprocess_data(df)\n",
    "label_encode_nominal_data(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- Hyperparameter Tuning ---\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20, None],           # this Limit tree depth\n",
    "    'min_samples_split': [2, 5, 10],              # sets min samples to split a node\n",
    "    'min_samples_leaf': [1, 2, 5]                 # sets min samples per leaf\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid,\n",
    "                           cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1)\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "tuning_time = time.time() - start_time\n",
    "\n",
    "# Best model from grid search\n",
    "best_dt = grid_search.best_estimator_\n",
    "print(\"\\nBest Hyperparameters:\", grid_search.best_params_)\n",
    "print(f\"Hyperparameter Tuning Time: {tuning_time:.4f} seconds\")\n",
    "\n",
    "# --- Feature Selection ---\n",
    "selector = SelectKBest(score_func=f_classif, k='all')  # Start with all features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get feature scores and select top features\n",
    "feature_scores = pd.DataFrame({'Feature': X_train.columns, 'Score': selector.scores_})\n",
    "feature_scores = feature_scores.sort_values(by='Score', ascending=False)\n",
    "print(\"\\nFeature Importance Scores (ANOVA F-value):\")\n",
    "print(feature_scores)\n",
    "\n",
    "k_values = [10, 20, 30]\n",
    "best_k = 0\n",
    "best_f1 = 0\n",
    "best_X_train_selected = None\n",
    "best_X_test_selected = None\n",
    "\n",
    "for k in k_values:\n",
    "    top_features = feature_scores['Feature'].head(k).tolist()\n",
    "    X_train_selected = X_train[top_features]\n",
    "    X_test_selected = X_test[top_features]\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_dt.fit(X_train_selected, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    y_train_pred = best_dt.predict(X_train_selected)\n",
    "    y_test_pred = best_dt.predict(X_test_selected)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "    if test_f1 > best_f1:\n",
    "        best_f1 = test_f1\n",
    "        best_k = k\n",
    "        best_X_train_selected = X_train_selected\n",
    "        best_X_test_selected = X_test_selected\n",
    "\n",
    "# Final training with best features and hyperparameters\n",
    "start_time = time.time()\n",
    "best_dt.fit(best_X_train_selected, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "y_train_pred = best_dt.predict(best_X_train_selected)\n",
    "y_test_pred = best_dt.predict(best_X_test_selected)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print results with best configuration\n",
    "print(f\"\\nBest Feature Configuration: Top {best_k} features\")\n",
    "print(\"Selected Features:\", best_X_train_selected.columns.tolist())\n",
    "print_score(y_test, y_test_pred, training_time, train_accuracy, test_accuracy)\n",
    "check_fitting(train_accuracy, test_accuracy)"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import plot_tree\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# --- Visualize the Decision Tree ---\n",
    "plt.figure(figsize=(20, 10))  # Adjust size for readability\n",
    "plot_tree(best_dt,\n",
    "          feature_names=best_X_train_selected.columns.tolist(),  # Use selected features\n",
    "          class_names=np.unique(y).tolist(),                     # Unique attack categories\n",
    "          filled=True,                                           # Color by class\n",
    "          rounded=True,                                          # Rounded boxes\n",
    "          fontsize=10,                                           # Font size\n",
    "          max_depth=3)                                           # Limit depth for clarity\n",
    "plt.title(\"Decision Tree Visualization (Top 3 Levels)\", fontsize=14)\n",
    "plt.show()"
   ],
   "id": "c1c94138688e2d33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ROC Curve of this Model:",
   "id": "15d7fcfcd6c71d20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Get unique classes\n",
    "classes = np.unique(y)\n",
    "n_classes = len(classes)\n",
    "\n",
    "# Binarize the test labels\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_test_prob = best_dt.predict_proba(best_X_test_selected)\n",
    "\n",
    "# Compute ROC curve and ROC AUC for each class\n",
    "fpr = dict()  # False Positive Rate\n",
    "tpr = dict()  # True Positive Rate\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_test_prob[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple']  # Adjust based on number of classes\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'ROC curve for {classes[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "# Plot diagonal line (random guessing)\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves - One vs Rest')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print AUC scores for each class\n",
    "print(\"\\nROC AUC Scores for Each Class:\")\n",
    "for i in range(n_classes):\n",
    "    print(f\"{classes[i]}: {roc_auc[i]:.4f}\")"
   ],
   "id": "ce959fb36206d2f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
